{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"data_part1/Data_All_Variables_2019_2024\"  # Remplacer par votre chemin\n",
    "DB_CONFIG = {\n",
    "    'postgresql': 'postgresql://user:password@localhost:5432/db_name',\n",
    "    'mysql': 'mysql+pymysql://user:password@localhost:3306/db_name'\n",
    "}\n",
    "OUTPUT_CSV = \"./meteo_data_cleaned.csv\"\n",
    "TABLE_NAME = \"meteo_data\"\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('MeteoETL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# √âtape 2 : Chargement des donn√©es\n",
    "# --------------------------------------------------\n",
    "def load_data(directory):\n",
    "    \"\"\"Charge les donn√©es CSV depuis les sous-dossiers annuels\"\"\"\n",
    "    logger.info(\"D√©marrage du chargement des donn√©es...\")\n",
    "    \n",
    "    all_dfs = []\n",
    "    years = [str(y) for y in range(2019, 2025)]  # 2019 √† 2024\n",
    "    \n",
    "    for year in tqdm(years, desc=\"Ann√©es\"):\n",
    "        year_path = os.path.join(directory, year)\n",
    "        if not os.path.exists(year_path):\n",
    "            logger.warning(f\"Dossier {year} non trouv√© - Ignor√©\")\n",
    "            continue\n",
    "            \n",
    "        for file in tqdm(os.listdir(year_path), desc=f\"Fichiers {year}\", leave=False):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(year_path, file)\n",
    "                try:\n",
    "                    # Chargement en sp√©cifiant les colonnes\n",
    "                    df = pd.read_csv(\n",
    "                        file_path,\n",
    "                        header=None,\n",
    "                        usecols=[1, 2, 3, 4, 5],\n",
    "                        names=['datetime', 'temperature', 'irradiance', 'humidity', 'wind_speed']\n",
    "                    )\n",
    "                    all_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erreur sur {file_path}: {str(e)}\")\n",
    "    \n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"Aucune donn√©e charg√©e - V√©rifiez le chemin des donn√©es\")\n",
    "    \n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    logger.info(f\"Chargement termin√© : {len(full_df)} lignes charg√©es\")\n",
    "    return full_df\n",
    "\n",
    "# --------------------------------------------------\n",
    "# √âtape 3 : Nettoyage des donn√©es\n",
    "# --------------------------------------------------\n",
    "def clean_data(df):\n",
    "    \"\"\"Nettoie et transforme le dataframe\"\"\"\n",
    "    logger.info(\"D√©marrage du nettoyage des donn√©es...\")\n",
    "    \n",
    "    # Conversion des dates (format jour/mois)\n",
    "    df['datetime'] = pd.to_datetime(\n",
    "        df['datetime'],\n",
    "        format='%d/%m/%Y %H:%M:%S.%f',\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Suppression des dates invalides\n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=['datetime'])\n",
    "    logger.info(f\"Dates invalides supprim√©es : {initial_count - len(df)} lignes\")\n",
    "    \n",
    "    # Filtrage des valeurs aberrantes\n",
    "    df = df[\n",
    "        (df['temperature'].between(-50, 60)) &\n",
    "        (df['irradiance'] >= 0) &\n",
    "        (df['humidity'].between(0, 100)) &\n",
    "        (df['wind_speed'] >= 0)\n",
    "    ]\n",
    "    logger.info(f\"Valeurs aberrantes supprim√©es : {initial_count - len(df)} lignes\")\n",
    "    \n",
    "    # Suppression des doublons temporels\n",
    "    df = df.sort_values('datetime')\n",
    "    df = df.drop_duplicates(subset=['datetime'], keep='last')\n",
    "    logger.info(f\"Doublons temporels supprim√©s : {initial_count - len(df)} lignes\")\n",
    "    \n",
    "    # Arrondissement des valeurs\n",
    "    df = df.round({\n",
    "        'temperature': 1,\n",
    "        'irradiance': 3,\n",
    "        'humidity': 1,\n",
    "        'wind_speed': 2\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Donn√©es nettoy√©es : {len(df)} lignes restantes\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# √âtape 4 : Sauvegarde dans les bases de donn√©es\n",
    "# --------------------------------------------------\n",
    "def save_to_db(df, db_type, table_name=TABLE_NAME):\n",
    "    \"\"\"Sauvegarde les donn√©es dans la base sp√©cifi√©e\"\"\"\n",
    "    logger.info(f\"D√©marrage de la sauvegarde dans {db_type.upper()}...\")\n",
    "    \n",
    "    try:\n",
    "        engine = create_engine(DB_CONFIG[db_type])\n",
    "        \n",
    "        # Cr√©ation de la table avec types optimis√©s\n",
    "        dtype_map = {\n",
    "            'datetime': 'TIMESTAMP PRIMARY KEY',\n",
    "            'temperature': 'FLOAT',\n",
    "            'irradiance': 'FLOAT',\n",
    "            'humidity': 'FLOAT',\n",
    "            'wind_speed': 'FLOAT'\n",
    "        }\n",
    "        \n",
    "        # Insertion par batch\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            chunksize=10000,\n",
    "            dtype=dtype_map,\n",
    "            method='multi'  # Insertion multi-lignes\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Donn√©es sauvegard√©es avec succ√®s dans {db_type.upper()}!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur DB {db_type}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# %%\n",
    "# --------------------------------------------------\n",
    "# √âtape 5 : Pipeline complet\n",
    "# --------------------------------------------------\n",
    "def run_pipeline():\n",
    "    \"\"\"Ex√©cute le pipeline ETL complet\"\"\"\n",
    "    try:\n",
    "        # Extraction\n",
    "        raw_df = load_data(DATA_DIR)\n",
    "        \n",
    "        # Transformation\n",
    "        cleaned_df = clean_data(raw_df)\n",
    "        \n",
    "        # Chargement\n",
    "        save_to_db(cleaned_df, 'postgresql')\n",
    "        save_to_db(cleaned_df, 'mysql')\n",
    "        \n",
    "        # Sauvegarde locale\n",
    "        cleaned_df.to_csv(OUTPUT_CSV, index=False)\n",
    "        logger.info(f\"Sauvegarde CSV locale : {OUTPUT_CSV}\")\n",
    "        \n",
    "        # Rapport final\n",
    "        logger.info(\"TRAITEMENT TERMIN√â AVEC SUCC√àS!\")\n",
    "        return cleaned_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.exception(\"ERREUR CRITIQUE DANS LE PIPELINE\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initiale termin√©e\n",
      "üìÅ Dossier de donn√©es: data_part1/Data_All_Variables_2019_2024\n",
      "üìÖ Ann√©es √† traiter: 2019, 2020, 2021, 2022, 2023, 2024\n"
     ]
    }
   ],
   "source": [
    "YEARS = [str(y) for y in range(2019, 2025)]  # 2019-2024\n",
    "\n",
    "print(\"‚úÖ Configuration initiale termin√©e\")\n",
    "print(f\"üìÅ Dossier de donn√©es: {DATA_DIR}\")\n",
    "print(f\"üìÖ Ann√©es √† traiter: {', '.join(YEARS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Exploration de la structure des fichiers...\n",
      " - 2019: 13 fichiers CSV\n",
      " - 2020: 12 fichiers CSV\n",
      " - 2021: 13 fichiers CSV\n",
      " - 2022: 12 fichiers CSV\n",
      " - 2023: 12 fichiers CSV\n",
      " - 2024: 11 fichiers CSV\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 2. Exploration de la structure des fichiers\n",
    "# --------------------------------------------------\n",
    "print(\"\\nüîç Exploration de la structure des fichiers...\")\n",
    "\n",
    "file_counts = {}\n",
    "for year in YEARS:\n",
    "    year_path = os.path.join(DATA_DIR, year)\n",
    "    if os.path.exists(year_path):\n",
    "        num_files = len([f for f in os.listdir(year_path) if f.endswith('.csv')])\n",
    "        file_counts[year] = num_files\n",
    "        print(f\" - {year}: {num_files} fichiers CSV\")\n",
    "    else:\n",
    "        print(f\" - {year}: Dossier introuvable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fichier exemple: data_part1/Data_All_Variables_2019_2024/2019/140419-14052019AllVariables.csv\n",
      "\n",
      "üìä Structure brute du fichier:\n",
      "- Dimensions: 1444 lignes x 5 colonnes\n",
      "- Aper√ßu des donn√©es:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FechaHora</th>\n",
       "      <th>ROOT.meteoPDL.AirTemperature.hf</th>\n",
       "      <th>ROOT.meteoPDL.Pyr1IrradianceCompensated.hf</th>\n",
       "      <th>ROOT.meteoPDL.RelativeHumidity.hf</th>\n",
       "      <th>ROOT.meteoPDL.WindSpeed.hf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14/04/2019 00:00:59.000</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>0.02737</td>\n",
       "      <td>17.251152</td>\n",
       "      <td>0.847570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14/04/2019 00:30:59.000</td>\n",
       "      <td>31.874424</td>\n",
       "      <td>0.09228</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>1.893272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/04/2019 01:00:59.000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>2.095780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 FechaHora  ROOT.meteoPDL.AirTemperature.hf  \\\n",
       "0  14/04/2019 00:00:59.000                        31.700001   \n",
       "1  14/04/2019 00:30:59.000                        31.874424   \n",
       "2  14/04/2019 01:00:59.000                        32.000000   \n",
       "\n",
       "   ROOT.meteoPDL.Pyr1IrradianceCompensated.hf  \\\n",
       "0                                     0.02737   \n",
       "1                                     0.09228   \n",
       "2                                     0.00000   \n",
       "\n",
       "   ROOT.meteoPDL.RelativeHumidity.hf  ROOT.meteoPDL.WindSpeed.hf  \n",
       "0                          17.251152                    0.847570  \n",
       "1                          16.500000                    1.893272  \n",
       "2                          15.300000                    2.095780  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyse des colonnes:\n",
      "Colonne 0: 0 valeurs manquantes, 1444 valeurs uniques\n",
      "Colonne 1: 0 valeurs manquantes, 733 valeurs uniques\n",
      "Colonne 2: 0 valeurs manquantes, 1317 valeurs uniques\n",
      "Colonne 3: 0 valeurs manquantes, 1291 valeurs uniques\n",
      "Colonne 4: 0 valeurs manquantes, 1270 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 3. Chargement d'un fichier exemple\n",
    "# --------------------------------------------------\n",
    "sample_file = None\n",
    "for year in YEARS:\n",
    "    year_path = os.path.join(DATA_DIR, year)\n",
    "    if os.path.exists(year_path) and os.listdir(year_path):\n",
    "        sample_file = os.path.join(year_path, os.listdir(year_path)[0])\n",
    "        break\n",
    "\n",
    "if sample_file:\n",
    "    print(f\"\\nüìÇ Fichier exemple: {sample_file}\")\n",
    "    \n",
    "    # Chargement sans traitement\n",
    "    sample_df = pd.read_csv(sample_file, delimiter=';')\n",
    "    sample_df = sample_df.iloc[:, :-1]\n",
    "    \n",
    "    print(\"\\nüìä Structure brute du fichier:\")\n",
    "    print(f\"- Dimensions: {sample_df.shape[0]} lignes x {sample_df.shape[1]} colonnes\")\n",
    "    print(\"- Aper√ßu des donn√©es:\")\n",
    "    display(sample_df.head(3))\n",
    "    \n",
    "    # V√©rification des colonnes\n",
    "    print(\"\\nüîç Analyse des colonnes:\")\n",
    "    for i in range(sample_df.shape[1]):\n",
    "        col_data = sample_df.iloc[:, i]\n",
    "        null_count = col_data.isnull().sum()\n",
    "        unique_count = col_data.nunique()\n",
    "        print(f\"Colonne {i}: {null_count} valeurs manquantes, {unique_count} valeurs uniques\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun fichier CSV trouv√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FechaHora', 'ROOT.meteoPDL.AirTemperature.hf',\n",
       "       'ROOT.meteoPDL.Pyr1IrradianceCompensated.hf',\n",
       "       'ROOT.meteoPDL.RelativeHumidity.hf', 'ROOT.meteoPDL.WindSpeed.hf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöö D√©but du chargement des donn√©es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ann√©es: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 14.95it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ùå Aucune donn√©e charg√©e!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m             problem_files.append((file_path, \u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_dfs:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå Aucune donn√©e charg√©e!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m meteo_df = pd.concat(all_dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Chargement termin√©: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(meteo_df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lignes charg√©es\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: ‚ùå Aucune donn√©e charg√©e!"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 4. Chargement complet des donn√©es\n",
    "# --------------------------------------------------\n",
    "print(\"\\nüöö D√©but du chargement des donn√©es...\")\n",
    "\n",
    "all_dfs = []\n",
    "problem_files = []\n",
    "\n",
    "for year in tqdm(YEARS, desc=\"Ann√©es\"):\n",
    "    year_path = os.path.join(DATA_DIR, year)\n",
    "    if not os.path.exists(year_path):\n",
    "        print(f\"‚ö†Ô∏è Dossier {year} introuvable - Ignor√©\")\n",
    "        continue\n",
    "        \n",
    "    year_files = [f for f in os.listdir(year_path) if f.endswith('.csv')]\n",
    "    for file in tqdm(year_files, desc=f\"Fichiers {year}\", leave=False):\n",
    "        file_path = os.path.join(year_path, file)\n",
    "        try:\n",
    "            # Chargement des 6 colonnes attendues\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                delimiter=';'\n",
    "            )\n",
    "            df = df.iloc[:, :-1]\n",
    "            df.rename(columns=['datetime', 'airtemperature', 'irradiance', 'humidity', 'wind_speed'])\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            problem_files.append((file_path, str(e)))\n",
    "    \n",
    "if not all_dfs:\n",
    "    raise ValueError(\"‚ùå Aucune donn√©e charg√©e!\")\n",
    "\n",
    "meteo_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\n‚úÖ Chargement termin√©: {len(meteo_df):,} lignes charg√©es\")\n",
    "print(f\"‚ö†Ô∏è {len(problem_files)} fichiers avec erreurs\")\n",
    "\n",
    "# Affichage des probl√®mes\n",
    "if problem_files:\n",
    "    print(\"\\nProbl√®mes rencontr√©s:\")\n",
    "    for i, (file, error) in enumerate(problem_files[:3]):\n",
    "        print(f\" - {file}: {error}\")\n",
    "    if len(problem_files) > 3:\n",
    "        print(f\" - ... et {len(problem_files)-3} autres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>airtemperature</th>\n",
       "      <th>irradiance</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.700001</td>\n",
       "      <td>0.027370</td>\n",
       "      <td>17.251152</td>\n",
       "      <td>0.847570</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.874424</td>\n",
       "      <td>0.092280</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>1.893272</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>2.095780</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.981928</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>15.171184</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.387892</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>21.265627</td>\n",
       "      <td>2.242200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.071901</td>\n",
       "      <td>2.683918</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.368471</td>\n",
       "      <td>19.300001</td>\n",
       "      <td>0.697779</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.961927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.338074</td>\n",
       "      <td>1.557688</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30.428099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.343803</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30.200001</td>\n",
       "      <td>0.096841</td>\n",
       "      <td>14.770206</td>\n",
       "      <td>1.355424</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime airtemperature irradiance  humidity  wind_speed\n",
       "0  31.700001       0.027370  17.251152  0.847570         NaN\n",
       "1  31.874424       0.092280  16.500000  1.893272         NaN\n",
       "2  32.000000       0.000000  15.300000  2.095780         NaN\n",
       "3  31.981928       0.021053  15.171184  2.000000         NaN\n",
       "4  28.387892       0.570000  21.265627  2.242200         NaN\n",
       "5  30.500000       0.000000  17.071901  2.683918         NaN\n",
       "6  29.000000       0.368471  19.300001  0.697779         NaN\n",
       "7  29.961927       0.000000  16.338074  1.557688         NaN\n",
       "8  30.428099       0.000000  15.343803  1.100000         NaN\n",
       "9  30.200001       0.096841  14.770206  1.355424         NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
